{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"https://raw.githubusercontent.com/codeforcauseorg/ML-Bootcamp-July/master/datasets/bayes/spam.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham',\n",
       "       'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "       nan, nan, nan], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 1]\n",
    "y = data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5572,), (5572,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+')   #To extract all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))   #contains stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStem(review):   #removes all the stem words.\n",
    "    \n",
    "    review = review.lower()   \n",
    "    tokens = tokenizer.tokenize(review)   #breaking into stopwords.\n",
    "    \n",
    "    removed_stopwords = [w for w in tokens if w not in sw]    #this contains the words removed from stopwords.\n",
    "    stemmed_words = [ps.stem(token) for token in removed_stopwords]   #stemming converts different forms of same words into single word.\n",
    "    \n",
    "    clean_review = ' '.join(stemmed_words)   #joining the stemmed words\n",
    "    return clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a clean document \n",
    "def getDoc(document):\n",
    "    \n",
    "    d = []\n",
    "    for doc in document:\n",
    "        d.append(getStem(doc))\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_doc = getDoc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary creation\n",
    "vc = cv.fit_transform(stemmed_doc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB from sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977705274605764"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputting the data\n",
    "\n",
    "messages = [\"\"\"\n",
    "            Hi Lavisha,\n",
    "\n",
    "So, here's the deal:\n",
    "I'm writing a brand new book on Optical Character Recognition, Tesseract, and OpenCV.\n",
    "I'll be launching an IndieGoGo campaign in two weeks, to offer pre-orders of the book at a significantly discounted price.\n",
    "Over the coming weeks, I'll be sharing more details on the book.\n",
    "But in the meantime, I wanted to give you a high-level overview, and address some common questions I know I will receive.\n",
    "\"\"\",\n",
    "           \"\"\"\n",
    "           Lavisha, Bachelor of Technology (B.Tech) students have a chance to earn while studying in NIT Hamirpur by working with the world's largest student program - Internshala Student Partner (ISP). Develop your marketing and communication skills while you earn huge rewards!\n",
    "\n",
    "Rewards - We are giving away financial rewards worth INR 25 lacs+ to our student partners. Apart from this, you can also win super reward iPhone 11 and many more\n",
    "\n",
    "Eligibility - Any student from any degree, stream, and year of study can apply\n",
    "\n",
    "Deadline - 19th August 2020. Click here or on the button below to apply\n",
    "           \"\"\",\n",
    "           \"\"\"Hello CodeCheffer,\n",
    "\n",
    "We hope you are hale, hearty, and coding. In case you forgot to mark your calendars, we just wanted to remind you that the August Long Challenge is almost here.\n",
    "\n",
    "Our setters have gone great lengths to come up with innovative problems, and this contest promises a vibrant start to the month.\n",
    "\n",
    "Contest details for the fanatics:\n",
    "Start date: 7th August 15:00 hrs IST(check your timezone here)\n",
    "Contest duration: 10 mind-boggling days.\n",
    "Compete Now\n",
    "\n",
    "Keep a close watch on our Twitter page for teasers to the Long Challenge problems. Donâ€™t miss out on the fun!\n",
    "\n",
    "More exciting News\n",
    "\n",
    "We are launching a brand-new YouTube channel called \"Learn Competitive Programming with CodeChef.\" The channel will be filled with content from educators across the globe, explaining how to solve popular CodeChef problems. So go and hit the subscribe button, because this is a ride you definitely want to be a part of.\n",
    "           \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the text\n",
    "def prepare(messages):\n",
    "    d = getDoc(messages)\n",
    "    \n",
    "    #dont use fit_transform here while testing as it creates a new vocabulary. Use transform only.\n",
    "    return cv.transform(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prepare(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
